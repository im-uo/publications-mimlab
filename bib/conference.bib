@inproceedings{liu2025paladin,
    author = {Hong Liu and Yuta Nakashima and Noboru Babaguchi},
    title = {{PALADIN}: Understanding Video Intentions in Political Advertisement Videos},
    booktitle = {Proc.~IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year = {2025},
    month = 2,
    addendum = {(Accepted)},
    note = {11 pages},
    abstract = {In this paper, we introduce a novel task for video understanding that focuses on detecting editing intentions in political advertisement videos. Political advertisement videos are edited with some intentions (e.g., ``associating some candidates with negative emotions'') of making people unthinkingly believe the messages in the videos, potentially ending up with some irrational bias.  Detecting such intentions is thus the primary step toward fairer decision-making based on the messages themselves. To this end, we classify such editing intentions into 10 categories (referred to as communication techniques) in consultation with a professional editor as well as based on communication techniques presented in the natural language processing community, and build a dataset of 12,526 political advertisement videos, each of which are annotated with several communication technique segments. We also explore the capability of existing video understanding models in detecting editing intentions over the dataset, which identifies new dimensions of challenges to be addressed. },
    organization = {(IEEE, NJ, USA)},
    entrysubtype = {conference},
}

@inproceedings{ramos2025nada,
    author = {Patrick Ramos and Nicolas Gonthier and Selina Khan and Yuta Nakashima and Noa Garcia},
    title = {No Annotations for Object Detection in Art through Stable Diffusion},
    booktitle = {Proc.~IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year = {2025},
    month = 2,
    addendum = {(Accepted)},
    note = {10 pages},
    abstract = {Object detection in art is a valuable tool for the digital humanities, as it allows for a faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art.},
    organization = {(IEEE, NJ, USA)},
    entrysubtype = {conference},
}


@inproceedings{zhang2024microemo,
    author = {Liyun Zhang and Zhaojie Luo amd Shuqiong Wu and Yuta Nakashima},
    title = {MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues},
    booktitle = {Proc.~2nd International Workshop on Multimodal and Responsible Affective Computing},
    year = {2024},
    month = 10,
    pages = {110--115},
    doi = {https://doi.org/10.1145/3689092.3689408},
    abstract = {Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal emotion recognition capabilities, integrating multimodal cues from visual, acoustic, and linguistic contexts in the video to recognize human emotional states. However, existing methods have limitations. They neglect to capture the dynamics of local subtle clues in facial features and also do not leverage the contextual dependencies of the utterance-aware temporal segments in videos. These shortcomings somewhat restrict their expected effectiveness. In this work, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention to the local facial clue dynamics and the contextual dependencies of utterance-aware video clips. Our model incorporates two key architectural contributions: (1) a global-local attention visual encoder that integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of subtle clues; (2) an utterance-aware video Q-Former that captures multi-scale and contextual dependencies by generating visual token sequences for each utterance segment and for the entire video then combining them. The experiments demonstrate that in a new Explainable Multimodal Emotion Recognition (EMER) task that exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner, MicroEmo demonstrates its effectiveness compared with the latest methods.},
    entrysubtype = {conference},
}

@inproceedings{wu2024exposed,
    author = {Yankun Wu and Yuta Nakashima and Noa Garcia},
    title = {Stable Diffusion Exposed: Gender Bias from Prompt to Image},
    booktitle = {Proc.~AAAI/ACM Conference on AI, Ethics, and Society},
    year = {2024},
    month = 10,
    addendum = {(DOIなし)},
    pages = {1648--1659},
    abstract = {Several studies have raised awareness about social biases in image generative models, demonstrating their predisposition towards stereotypes and imbalances. This paper contributes to this growing body of research by introducing an evaluation protocol that analyzes the impact of gender indicators at every step of the generation process on Stable Diffusion images. Leveraging insights from prior work, we explore how gender indicators not only affect gender presentation but also the representation of objects and layouts within the generated images. Our findings include the existence of differences in the depiction of objects, such as instruments tailored for specific genders, and shifts in overall layouts. We also reveal that neutral prompts tend to produce images more aligned with masculine prompts than their feminine counterparts. We further explore where bias originates through representational disparities and how it manifests in the images via prompt-image dependencies, and provide recommendations for developers and users to mitigate potential bias in image generation.},
    entrysubtype = {conference},
}

@inproceedings{wang2024direct,
    author = {Bowen Wang and Jiuyang Chang and Yiming Qian and Guoxin Chen and Junhao Chen and Zhouqiang Jiang and Jiahao Zhang and Yuta Nakashima and Hajime Nagahara},
    title = {{DiReCT}: Diagnostic Reasoning for Clinical Notes via Large Language Models},
    booktitle = {Proc.~Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
    year = {2024},
    month = 12,
    note = {14 pages},
    addendum = {(Accepted, \textbf{採択率: 25.3\%})},
    abstract = {Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios.},
    entrysubtype = {conference},
}

@inproceedings{hirota2024resampled,
    author = {Yusuke Hirota and Jerone TA Andrew and Dora Zhao and Orestis Papakyriakopoulos and Apostolos Modas and Yuta Nakashima and Alice Xiang},
    title = {Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes},
    booktitle = {Proc.~2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year = {2024},
    month = 11,
    pages = {8249--8267},
    addendum = {(Accepted, \textbf{採択率: 20.8\%})},
    abstract = {We tackle societal bias in image-text datasets by removing spurious correlations between protected groups and image attributes. Traditional methods only target labeled attributes, ignoring biases from unlabeled ones. Using text-guided inpainting models, our approach ensures protected group independence from all attributes and mitigates inpainting biases through data filtering. Evaluations on multi-label image classification and image captioning tasks show our method effectively reduces bias without compromising performance across various models.},
    entrysubtype = {conference},
}

@inproceedings{hirota2024from,
    author = {Yusuke Hirota and Ryo Hachiuma and Chao-Han Huck Yang and Yuta Nakashima},
    title = {From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment},
    booktitle = {Proc.~2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year = {2024},
    month = 11,
    pages = {17807--17816},
    addendum = {(Accepted, \textbf{採択率: 20.8\%})},
    abstract = {Large language models (LLMs) have enhanced the capacity of vision-language models to caption visual text. This generative approach to image caption enrichment further makes textual captions more descriptive, improving alignment with the visual context. However, while many studies focus on benefits of generative caption enrichment (GCE), are there any negative side effects? We compare standard-format captions and recent GCE processes from the perspectives of ``gender bias'' and ``hallucination'', showing that enriched captions suffer from increased gender bias and hallucination. Furthermore, models trained on these enriched captions amplify gender bias by an average of 30.9\% and increase hallucination by 59.5\%. This study serves as a caution against the trend of making captions more descriptive.},
    entrysubtype = {conference},
}

@inproceedings{leu2024auditingnsfw,
    author = {Warren Leu and Yuta Nakashima and Noa Garcia},
    title = {Auditing Image-based {NSFW} Classifiers for Content Filtering},
    booktitle = {Proc.~ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
    year = {2024},
    month = 6,
    pages = {1163--1173},
    doi = {https://doi.org/10.1145/3630106.3658963},
    url = {https://dl.acm.org/doi/abs/10.1145/3630106.3658963},
    abstract = {This paper examines NSFW (Not Safe For Work) image classifiers for content filtering. Through an audit of three prevalent NSFW classifiers, we analyze the relationship between NSFW predictions and three demographic factors: gender, skin-tone, and age. Our study reveals that women are disproportionately more frequently misclassified as NSFW compared to men, even when they appear conducting common daily-life activities. Additionally, we find that NSFW classifiers tend to mispredict images of people with lighter skin-tones and images depicting younger people. We explore the causes of such mispredictions by analyzing the explanatory pixel maps, which reveal some of the reasons behind the misclassifications. Overall, the implications of our findings become particularly salient when considering the application of filters based on NSFW classifiers, which we identified to have a direct impact on image datasets, computer vision models, generative AI, user experience, and artistic creativity. In summary, we hope our study brings attention to the inherent biases within NSFW classifiers and underscores the importance of addressing these issues to ensure fair and equitable outcomes in content filtering.},
    entrysubtype = {conference},

}

@inproceedings{chen2024emotion,
    author = {Tianwei Chen and Noa Garcia and Liangzhi Li and Yuta Nakashima},
    title = {Retrieving Emotional Stimuli in Artworks},
    booktitle = {Proc.~ 2024 International Conference on Multimedia Retrieval (ICMR)},
    year = {2024},
    month = 5,
    pages = {515--523},
    doi = {https://doi.org/10.1145/3652583.3658102},
    url = {https://dl.acm.org/doi/abs/10.1145/3652583.3658102},
    abstract = {We introduce an emotional stimuli retrieval task that targets extracting emotional regions that evoke people's emotions (i.e., emotional stimuli) in artworks. This task offers new challenges to the community because of the diversity of artwork styles and the subjectivity of emotions, which can be a suitable testbed for benchmarking the capability of the current neural networks to deal with human emotion. For this task, we construct a dataset called APOLO for quantifying emotional stimuli retrieval performance in artworks by crowd-sourcing pixel-level annotation of emotional stimuli. APOLO contains 6,781 emotional stimuli in 4,718 artworks for validation and testing. We also evaluate eight baseline methods, including a dedicated one, to show the difficulties of the task and the limitations of the current techniques through qualitative and quantitative experiments. Our data and methods are available in \url{https://github.com/Tianwei3989/apolo}.},
    entrysubtype = {conference},
}

@inproceedings{wu2025reproducibility,
    author = {Yankun Wu and Yuta Nakashima and Noa Garcia and Sheng Li and Zhaoyang Zeng},
    title = {Reproducibility Companion Paper: Stable Diffusion for Content-Style Disentanglement in Art Analysis},
    booktitle = {Proc.~ 2024 International Conference on Multimedia Retrieval (ICMR)},
    year = 2024,
    month = 5,
    pages = {1228--1231},
    doi = {https://doi.org/10.1145/3652583.3658372},
    url = {https://dl.acm.org/doi/abs/10.1145/3652583.3658372},
    abstract = {In this companion paper, we provide the artifacts of the GOYA model for disentangling content and style in art paintings, as presented at ICMR2023. The scripts are written in Python.},
    entrysubtype = {conference},
}

@inproceedings{chen2024future,
    author = {Tianwei Chen and Yusuke Hirota and Mayu Otani and Noa Garcia and Yuta Nakashima},
    title = {Would Deep Generative Models Amplify Bias in Future Models?},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2024},
    month = 6,
    pages = {pp.~10833--10843},
    addendum = {(\textbf{採択率: 23.6\%})},
    url = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Would_Deep_Generative_Models_Amplify_Bias_in_Future_Models_CVPR_2024_paper.html},
    doi = {https://doi.org/10.1109/CVPR52733.2024.01030},
    abstract = {We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images concerns arise regarding inherent biases that may accompany them potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop resulting in bias amplification would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models which we evaluate in terms of quality and bias. Contrary to expectations our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena such as artifacts in image generation (e.g. blurry faces) or pre-existing biases in the original datasets.},
    entrysubtype = {conference},
}

@inproceedings{pang2024revisiting,
    author = {Zongshang Pang and Yuta Nakashima and Mayu Otani and Hajime Nagahara},
    title = {Revisiting pixel-level contrastive pre-training on scene images},
    booktitle = {Proc.~IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    year = {2024},
    month = 1,
    page = {1784--1793},
    doi = {https://doi.org/10.1109/WACV57701.2024.00180},
    url = {https://openaccess.thecvf.com/content/WACV2024/html/Pang_Revisiting_Pixel-Level_Contrastive_Pre-Training_on_Scene_Images_WACV_2024_paper.html},
    abstract = {Contrastive image representation learning through instance discrimination has shown impressive transfer performance. Recent strategies have focused on pushing the limit of their transfer performance for dense prediction tasks, particularly when conducting pre-training on scene images with complex structures. Initial approaches employ pixel-level contrastive pre-training to optimize dense spatial features, while subsequent methods utilize region-mining algorithms to capture holistic regional semantics and address the issue of semantically inconsistent scene image crops. In this paper, we revisit pixel-level contrastive pre-training on scene images. Contrary to the assumption that pixel-level learning falls short in achieving these objectives, we demonstrate its under-explored potentials: (1) it can effectively learn holistic regional semantics more simply compared to region-level methods, and (2) it intrinsically provides tools to mitigate the impact of semantically inconsistent views involved with scene-level training images. We propose PixCon, a pixel-level contrastive learning framework, and explore two variants with different positive matching strategies to investigate the potential of pixel-level learning. Additionally, when PixCon incorporates a novel semantic reweighting approach tailored for scene image pre-training, it outperforms or matches the performance of previous region-level methods in object detection and semantic segmentation tasks across multiple benchmarks.},
    entrysubtype = {conference},
}

@inproceedings{zhang2024instruct,
    author = {Jiahao Zhang and Bowen Wang and Liangzhi Li and Yuta Nakashima and Hajime Nagahara},
    title = {Instruct me more! {R}andom prompting for visual in-context learning},
    booktitle = {Proc.~IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    year = {2024},
    month = 1,
    pages = {2597--2606},
    doi = {https://doi.org/10.1109/WACV57701.2024.00258},
    url = {https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Instruct_Me_More_Random_Prompting_for_Visual_In-Context_Learning_WACV_2024_paper.html},
    abstract = {Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at \url{https://github.com/Jackieam/InMeMo}.},
    entrysubtype = {conference},
}

@inproceedings{zhao2023fakenews,
    author = {Zhao, Wanqing and Yuta Nakashima and Chen, Haiyuan and Babaguchi, Noboru}, 
    title = {Enhancing Fake News Detection in Social Media via Label Propagation on Cross-Modal Tweet Graph}, 
    year = {2023},
    url = {https://doi.org/10.1145/3581783.3612086}, 
    doi = {https://doi.org/10.1145/3581783.3612086}, 
    abstract = {Fake news detection in social media has become increasingly important due to the rapid proliferation of personal media channels and the consequential dissemination of misleading information. Existing methods, which primarily rely on multimodal features and graph-based techniques, have shown promising performance in detecting fake news. However, they still face a limitation, i.e., sparsity in graph connections, which hinders capturing possible interactions among tweets. This challenge has motivated us to explore a novel method that densifies the graph's connectivity to capture denser interaction better. Our method constructs a cross-modal tweet graph using CLIP, which encodes images and text into a unified space, allowing us to extract potential connections based on similarities in text and images. We then design a Feature Contextualization Network with Label Propagation (FCN-LP) to model the interaction among tweets as well as positive or negative correlations between predicted labels of connected tweets. The propagated labels from the graph are weighted and aggregated for the final detection. To enhance the model's generalization ability to unseen events, we introduce a domain generalization loss that ensures consistent features between tweets on seen and unseen events. We use three publicly available fake news datasets, Twitter, PHEME, and Weibo, for evaluation. Our method consistently improves the performance over the state-of-the-art methods on all benchmark datasets and effectively demonstrates its aptitude for generalizing fake news detection in social media.}, 
    booktitle = {Proc.~ACM International Conference on Multimedia (MM)}, 
    pages = {2400–2408},
    month = 9,
    entrysubtype = {conference},
}

@inproceedings{wu2023notonly,
    author = {Yankun Wu and Yuta Nakashima and Noa Garcia},
    title = {Not only generative art: Stable diffusion for content-style disentanglement in art analysis},
    booktitle = {Proc.~ 2023 ACM International Conference on Multimedia Retrieval (ICMR)},
    year = {2023},
    month = 6,
    note = {pp.~199--208},
    url = {https://dl.acm.org/doi/abs/10.1145/3591106.3592262},
    doi = {https://doi.org/10.1145/3591106.3592262},
    abstract = {The duality of content and style is inherent to the nature of art. For humans, these two elements are clearly different: content refers to the objects and concepts in the piece of art, and style to the way it is expressed. This duality poses an important challenge for computer vision. The visual appearance of objects and concepts is modulated by the style that may reflect the author’s emotions, social trends, artistic movement, etc., and their deep comprehension undoubtfully requires to handle both. A promising step towards a general paradigm for art analysis is to disentangle content and style, whereas relying on human annotations to cull a single aspect of artworks has limitations in learning semantic concepts and the visual appearance of paintings. We thus present GOYA, a method that distills the artistic knowledge captured in a recent generative model to disentangle content and style. Experiments show that synthetically generated images sufficiently serve as a proxy of the real distribution of artworks, allowing GOYA to separately represent the two elements of art while keeping more information than existing methods.},
    entrysubtype = {conference},
}

@inproceedings{wang2023learning,
    author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Hajime Nagahara},
    title = {Learning bottleneck concepts in image classification},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    note = {pp.~10962--10971},
    doi = {https://doi.org/10.1109/CVPR52729.2023.01055},
    addendum = {(\textbf{採択率: 25.3\%})},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.html},
    abstract = {Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI provides a way to address this challenge, mostly by providing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some recent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bottleneck Concept Learner (BotCL), which represents an image solely by the presence/absence of concepts learned through training over the target task without explicit supervision over the concepts. It uses self-supervision and tailored regularizers so that learned concepts can be human-understandable. Using some image classification tasks as our testbed, we demonstrate BotCL's potential to rebuild neural networks for better interpretability.},
    entrysubtype = {conference},
}

@inproceedings{hirota2023model,
    author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia},
    title = {Model-agnostic gender debiased image captioning},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    note = {pp.~15191--15200},
    month = 6,
    year = {2023},
    doi = {https://doi.org/10.1109/CVPR52729.2023.01458},
    addendum = {(\textbf{採択率: 25.3\%})},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hirota_Model-Agnostic_Gender_Debiased_Image_Captioning_CVPR_2023_paper.html},
    abstract = {Image captioning models are known to perpetuate and amplify harmful societal bias in the training set. In this work, we aim to mitigate such gender bias in image captioning models. While prior work has addressed this problem by forcing models to focus on people to reduce gender misclassification, it conversely generates gender-stereotypical words at the expense of predicting the correct gender. From this observation, we hypothesize that there are two types of gender bias affecting image captioning models: 1) bias that exploits context to predict gender, and 2) bias in the probability of generating certain (often stereotypical) words because of gender. To mitigate both types of gender biases, we propose a framework, called LIBRA, that learns from synthetically biased samples to decrease both types of biases, correcting gender misclassification and changing gender-stereotypical words to more neutral ones.},
    entrysubtype = {conference},
}

@inproceedings{garcia2023uncurated,
    author = {Noa Garcia and Yusuke Hirota and Yankun Wu and Yuta Nakashima},
    title = {Uncurated image-text datasets: Shedding light on demographic bias},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    note = {pp.~6957--6966},
    addendum = {(\textbf{採択率: 25.3\%})},
    doi = {https://doi.org/10.1109/CVPR52729.2023.00672},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.html},
    abstract = {The increasing tendency to collect large and uncurated datasets to train vision-and-language models has raised concerns about fair representations. It is known that even small but manually annotated datasets, such as MSCOCO, are affected by societal bias. This problem, far from being solved, may be getting worse with data crawled from the Internet without much control. In addition, the lack of tools to analyze societal bias in big collections of images makes addressing the problem extremely challenging. Our first contribution is to annotate part of the Google Conceptual Captions dataset, widely used for training vision-and-language models, with four demographic and two contextual attributes. Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented. Our last contribution lies in evaluating three prevailing vision-and-language tasks: image captioning, text-image CLIP embeddings, and text-to-image generation, showing that societal bias is a persistent problem in all of them.},
    entrysubtype = {conference},
}

@inproceedings{otani2023toward,
    author = {Mayu Otani and Riku Togashi and Yu Sawai and Ryosuke Ishigami and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"{a} and Shin’ichi Satoh},
    title = {Toward verifiable and reproducible human evaluation for text-to-image generation},
    booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2023},
    month = 6,
    note = {pp.~14277--14286},
    addendum = {(\textbf{採択率: 25.3\%})},
    doi = {https://doi.org/10.1109/CVPR52729.2023.01372},
    url = {https://openaccess.thecvf.com/content/CVPR2023/html/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.html},
    abstract = {Human evaluation is critical for validating the performance of text-to-image generative models, as this highly cognitive process requires deep comprehension of text and images. However, our survey of 37 recent papers reveals that many works rely solely on automatic measures (eg, FID) or perform poorly described human evaluations that are not reliable or repeatable. This paper proposes a standardized and well-defined human evaluation protocol to facilitate verifiable and reproducible human evaluation in future works. In our pilot data collection, we experimentally show that the current automatic measures are incompatible with human perception in evaluating the performance of the text-to-image generation results. Furthermore, we provide insights for designing human evaluation experiments reliably and conclusively. Finally, we make several resources publicly available to the community to facilitate easy and fast implementations.},
    entrysubtype = {conference},
}

@inproceedings{vo2022tone,
  title={Tone Classification for Political Advertising Video using Multimodal Cues},
  author={Anh-Khoa Vo and Yuta Nakashima},
  booktitle={Proc.~3rd ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
  note = {pp.~17--21},
  month = 6,
  year={2022},
  url = {https://dl.acm.org/doi/abs/10.1145/3512731.3534216},
  doi = {https://doi.org/10.1145/3512731.3534216},
  abstract = {Politics has always gotten much attention throughout history, and video advertisement has become one of the most essential tools for political communication. Analysis of such political advertising videos can provide more insight into the political campaign by evaluating the message in it, such as a candidate's attitude toward a certain political issue. In this paper, we propose to classify the tone in political advertising videos into promotive, contrastive, and their mixture using a deep neural network to benefit automatic analysis of such videos. We especially explore how different modalities of videos, i.e., visuals, audio, and text, contribute to improving the classification accuracy.},
    entrysubtype = {conference},
}

@inproceedings{verma2022multi,
  title={Multi-label disengagement and behavior prediction in online learning},
  author={Manisha Verma and Yuta Nakashima and Takemura, Noriko and Nagahara, Hajime},
  booktitle={Proc.~International Conference on Artificial Intelligence in Education},
  note = {pp.~633--639},
  month=7,
  year={2022},
  doi = {https://doi.org/10.1007/978-3-031-11644-5_60},
  url = {https://link.springer.com/chapter/10.1007/978-3-031-11644-5_60},
  abstract = {Student disengagement prediction in online learning environments is beneficial in various ways, especially to help provide timely cues to make some feedback or stimuli to the students. In this work, we propose a neural network-based model to predict students’ disengagement, as well as other behavioral cues, which might be relevant to students’ performance, using facial image sequences. For training and evaluating our model, we collected samples from multiple participants and annotated them with temporal segments of disengagement and other relevant behavioral cues with our multiple in-house annotators. We present prediction results of all behavior cues along with baseline comparison.},
    entrysubtype = {conference},
}

@inproceedings{teshima2022deep,
  title={Deep Gesture Generation for Social Robots Using Type-Specific Libraries},
  author={Teshima, Hitoshi and Wake, Naoki and Thomas, Diego and Yuta Nakashima and Kawasaki, Hiroshi and Ikeuchi, Katsushi},
  booktitle={Proc.~2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  note = {pp.~8286--8291},
  month=10,
  year={2022},
  doi = {https://doi.org/10.1109/IROS47612.2022.9981734},
  url = {https://ieeexplore.ieee.org/abstract/document/9981734},
  abstract = {Body language such as conversational gesture is a powerful way to ease communication. Conversational gestures do not only make a speech more lively but also contain semantic meaning that helps to stress important information in the discussion. In the field of robotics, giving conversational agents (humanoid robots or virtual avatars) the ability to properly use gestures is critical, yet remain a task of extraordinary difficulty. This is because given only a text as input, there are many possibilities and ambiguities to generate an appropriate gesture. Different to previous works we propose a new method that explicitly takes into account the gesture types to reduce these ambiguities and generate human-like conversational gestures. Key to our proposed system is a new gesture database built on the TED dataset that allows us to map a word to one of three types of gestures: “Imagistic” gestures, which express the content of the speech, “Beat” gestures, which emphasize words, and “No gestures.” We propose a system that first maps the words in the input text to their corresponding gesture type, generate type-specific gestures and combine the generated gestures into one final smooth gesture. In our comparative experiments, the effectiveness of the proposed method was confirmed in user studies for both avatar and humanoid robot.},
    entrysubtype = {conference},
}

@inproceedings{pang2023contrastive,
  title={Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization},
  author={Pang, Zongshang and Yuta Nakashima and Otani, Mayu and Nagahara, Hajime},
  booktitle={Proc.~IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  note = {pp.~2010--2019},
  month=1,
  year={2023},
  doi = {https://doi.org/10.1109/WACV56688.2023.00205},
  url = {https://openaccess.thecvf.com/content/WACV2023/html/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.html},
  abstract = {Video summarization aims to select a most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on an image classification task, the metrics can already yield high-quality importance scores, demonstrating better or competitive performance compared with past heavily-trained methods. We show that by refining the pre-trained features with contrastive learning, the frame-level importance scores can be further improved, and the model can learn from random videos and generalize to test videos with decent performance.},
    entrysubtype = {conference},
}

@inproceedings{suzuki2022emotional,
  title={Emotional Intensity Estimation based on Writer’s Personality},
  author={Suzuki, Haruya and Tarumoto, Sora and Kajiwara, Tomoyuki and Ninomiya, Takashi and Yuta Nakashima and Nagahara, Hajime},
  booktitle={Proc.~2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (AACL-IJCNJP): Student Research Workshop},
  note = {pp.~1--7},
  month=11,
  year={2022},
  addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{lemarchant2023inference,
  title={Inference Time Evidences of Adversarial Attacks for Forensic on Transformers},
  author={Lemarchant, Hugo and Li, Liangzi and Qian, Yiming and Yuta Nakashima and Nagahara, Hajime},
  booktitle={Proc.~AAAI-23 Workshop on Artificial Intelligence for Cyber Security (AICS)},
  month=2,
  note = {10 pages},
  year={2023},
  addendum = {(DOIなし)},
entrysubtype = {conference},
}

@inproceedings{hirota2022facct,
author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia},
title = {Gender and racial bias in visual question answering datasets},
booktitle = {Proc.~ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
month = 6,
year = {2022},
note = {pp.~1280--1292},
doi = {https://doi.org/10.1145/3531146.3533184},
abstract = {Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.},
    entrysubtype = {conference},
}

@inproceedings{suzuki2022japanese,
  title={A {J}apanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain},
  author={Suzuki, Haruya and Miyauchi, Yuto and Akiyama, Kazuki and Kajiwara, Tomoyuki and Ninomiya, Takashi and Takemura, Noriko and Yuta Nakashima and Nagahara, Hajime},
  booktitle={Proc.~Thirteenth Language Resources and Evaluation Conference (LREC)},
  note = {pp.~7022--7028},
  month = 6,
  year={2022},
  addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{hirota2022cvpr,
author = {Yusuke Hirota and Yuta Nakashima and Noa Garcia}, 
title = {Quantifying Societal Bias Amplification in Image Captioning}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2022},
month = 6,
note = {pp.~13440--13449},
addendum = {(\textbf{採択率: 25.3\%})},
doi = {https://doi.org/10.1109/CVPR52688.2022.01309},
    entrysubtype = {conference},
}


@inproceedings{otani2022cvpr,
author = {Mayu Otani and Riku Togashi and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a and Shin'ichi Satoh}, 
title = {Optimal Correction Cost for Object Detection Evaluation}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2022},
month = 6,
note = {pp.~21075--21083},
addendum = {(\textbf{採択率: 25.3\%})},
doi = {https://doi.org/10.1109/CVPR52688.2022.02043},
    entrysubtype = {conference},
}


@inproceedings{togashi2022cvpr,
author = {Riku Togashi and Mayu Otani and Yuta Nakashima and Esa Rahtu, Janne Heikkil\"a and Tetsuya Sakai}, 
title = {{AxIoU}: An Axiomatically Justified Measure for Video Moment Retrieval}, 
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2022},
month = 6,
note = {pp.~21044--21053},
addendum = {(\textbf{採択率: 25.3\%})},
doi = {https://doi.org/10.1109/CVPR52688.2022.02040},
    entrysubtype = {conference},

}

@inproceedings{teshima2022,
title = {Integration of gesture generation system using gesture library with {DIY} robot design kit},
author = {Hitoshi Teshima and Naoki Wake and Diego Thomas and Yuta Nakashima and David Baumert and Hiroshi Kawasaki and Katsushi Ikeuchi},
year = {2022},
month = 1,
booktitle = {Proc.~IEEE/SICE International Symposium on System Integration (SII)},
note = {pp.~361--366},
doi = {https://doi.org/10.1109/SII52469.2022.9708837},
    entrysubtype = {conference},
}

@inproceedings{shoji2021, 
title = {Museum Experience into a Souvenir: Generating Memorable Postcards from Guide Device Behavior Log},
author = {Yoshiyuki Shoji and Kenro Aihara and Noriko Kando and Yuta Nakashima and Hiroaki Ohshima and Shio Takidaira and Masaki Ueta and Takehiro Yamamoto and Yusuke Yamamoto},
year = {2021},
month = 9, 
booktitle = {Proc.~ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
note = {pp.~120-129},
doi = {https://doi.org/10.1109/JCDL52503.2021.00024},
    entrysubtype = {conference},
}

@inproceedings{nakashima2006estimation,
  title={Estimation of recording location using audio watermarking},
  author={Yuta Nakashima and Tachibana, Ryuki and Nishimura, Masafumi and Babaguchi, Noboru},
  booktitle={Proc.~Workshop on Multimedia and Security (MM\&Sec)},
  note ={pp.~108--113},
  year={2006},
  month = 9,
  doi = {https://doi.org/10.1145/1161366.1161385},
    entrysubtype = {conference},
}

@inproceedings{nakashima2010detecting,
  title={Detecting intended human objects in human-captured videos},
  author={Yuta Nakashima and Babaguchi, Noboru and Fan, Jianping},
  booktitle={Proc.~IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  note={pp.~33--40},
  year={2010},
  month = 6,
  doi = {https://doi.org/10.1109/CVPRW.2010.5543721},
    entrysubtype = {conference},
}

@inproceedings{nakashima2007determining,
  title={Determining Recording Location Based on Synchronization Positions of Audio watermarking},
  author={Yuta Nakashima and Tachibana, Ryuki and Nishimura, Masafumi and Babaguchi, Noboru},
  booktitle={Proc.~IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  volume={2},
  note={pp.~II-253--II-256},
  year={2007},
  month = 4, 
  doi = {https://doi.org/10.1109/ICASSP.2007.366220},
    entrysubtype = {conference},
}

@inproceedings{takehara2010digital,
  title={Digital diorama: Sensing-based real-world visualization},
  author={Takehara, Takumi and Yuta Nakashima and Nitta, Naoko and Babaguchi, Noboru},
  booktitle={Proc.~International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems},
  note={pp.~663--672},
  year={2010},
  month = 6,
  doi = {https://doi.org/10.1007/978-3-642-14058-7_68},
    entrysubtype = {conference},
}

@inproceedings{nakashima2007maximum,
  title={Maximum-likelihood estimation of recording position based on audio watermarking},
  author={Yuta Nakashima and Tachibana, Ryuki and Babaguchi, Noboru},
  booktitle={Proc.~International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIHMSP)},
  volume={2},
  note={pp.~255--258},
  year={2007},
  month = 11,
  doi = {https://doi.org/10.1109/IIH-MSP.2007.221},
    entrysubtype = {conference},
}

@inproceedings{wu2021transferring,
author = {Tianran Wu and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima and Haruo Takemura},
title = {Transferring domain-agnostic knowledge in video question answering},
booktitle = {Proc.~British Machine Vision Conference (BMVC)},
month = nov,
year = {2021},
note = {13 pages},
addendum = {(DOIなし)},
entrysubtype = {conference},
}

@inproceedings{vaigh202gcnboost,
author = {Cheikh Brahim El Vaigh and Noa Garcia and Benjamin Renoust and Chenhui Chu and Yuta Nakashima and Hajime Nagahara},
title = {{GCNBoost}: Artwork Classification by Label Propagation Through a Knowledge Graph},
booktitle = {Proc.~ACM International Conference on Multimedia Retrieval (ICMR)},
month = nov,
year = {2021},
note = {pp.~92--100},
doi = {https://doi.org/10.1145/3460426.3463636},
    entrysubtype = {conference},
}

@inproceedings{wang2021image,
author = {Bowen Wang and Liangzhi Li and Yuta Nakashima and Takehiro Yamamoto and Hiroaki Ohshima and Yoshiyuki Shoji and Kenro Aihara and Noriko Kando},
title = {Image Retrieval by Hierarchy-aware Deep Hashing Based on Multi-task Learning},
booktitle = {Proc.~ACM International Conference on Multimedia Retrieval (ICMR)},
month = nov,
year = {2021},
note = {pp.~486--490},
doi = {https://doi.org/10.1145/3460426.3463586},
    entrysubtype = {conference},
}

@inproceedings{qian2021built,
author = {Yiming Qian and Cheikh Brahim El Vaigh and Yuta Nakashima and Benjamin Renoust and Hajime Nagahara and Yutaka Fujioka},
title = {Built year prediction from Buddha face with heterogeneous labels},
booktitle = {Proc.~Workshop on Structuring and Understanding of Multimedia Heritage Contents (SUMAC)},
month = oct,
year = {2021},
note = {pp.~5--12},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}


@inproceedings{bai2021explain,
author = {Zechen Bai and Yuta Nakashima and Noa Garcia},
title = {Explain me the painting: Multi-topic knowledgeable art description generation},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision (ICCV)},
month = nov,
year = {2021},
note = {pp.~5422--5432},
addendum = {(\textbf{採択率: 26\%})},
doi = {https://doi.org/10.1109/ICCV48922.2021.00537},
    entrysubtype = {conference},
}

@inproceedings{li2021scouter,
author = {Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
title = {{SCOUTER}: Slot attention-based classifier for explainable image recognition},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision (ICCV)},
note = {pp.~1046--1055},
month = nov,
year = {2021},
addendum = {(\textbf{採択率: 26\%})},
doi = {https://doi.org/10.1109/ICCV48922.2021.00108},
    entrysubtype = {conference},
}

@inproceedings{hirota2021visual,
author = {Yusuke Hirota and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima and Ittetsu Taniguchi and Takao Onoye},
title = {Visual question answering with textual representations for images},
booktitle = {Proc.~IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
month = oct,
year = {2021},
note = {pp.~3154--3157},
doi = {https://doi.org/10.1109/ICCVW54120.2021.00353},
    entrysubtype = {conference},
}

@inproceedings{sayo2021posern,
author = {Akihiko Sayo and Diego Thomas and Hiroshi Kawasaki and Yuta Nakashima and Katsushi Ikeuchi},
title = {{PoseRN}: A {2D} pose refinement network for bias-free multi-view {3D} human pose estimation},
booktitle = {Proc.~International Conference on Image Processing (ICIP)},
month = sep,
year = {2021},
note = {pp.~3233--3237},
doi = {https://doi.org/10.1109/ICIP42928.2021.9506022},
    entrysubtype = {conference},
}

@inproceedings{verma2021learners,
author = {Manisha Verma and Yuta Nakashima and Hirokazu Kobori and Ryota Takaoka and Noriko Takemura and Tsukasa Kimura and Hajime Nagahara and Masayuki Numao and Kazumitsu Shinohara},
title = {Learners' efficiency prediction using facial behavior analysis},
booktitle = {Proc.~International Conference on Image Processing (ICIP)},
month = sep,
year = {2021},
note = {pp.~1084--1088},
doi = {https://doi.org/10.1109/ICIP42928.2021.9506203},
    entrysubtype = {conference},
} 

@inproceedings{samaran2021attending,
author = {Jules Samaran and Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima},
title = {Attending self-attention: A case study of visually grounded supervision in vision-and-language transformers},
booktitle = {Proc.~Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop},
month = aug,
year = {2021},
note = {pp.~81--86},
doi = {https://doi.org/10.18653/v1/2021.acl-srw.8},
    entrysubtype = {conference},
}

@inproceedings{kajiwara2021wrime,
author = {Tomoyuki Kajiwara and Chenhui Chu and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {{WRIME}: A new dataset for emotional intensity estimation with subjective and objective annotations},
booktitle = {Proc.~Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
month = jun,
year = {2021},
note = {pp.~2095--2104},
addendum = {(\textbf{採択率: 26\%})},
doi = {https://doi.org/10.18653/v1/2021.naacl-main.169},
    entrysubtype = {conference},
}

@inproceedings{wang2021mtunet,
author = {Bowen Wang and Liangzhi Li and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
title = {{MTUNet}: Few-shot image classification with visual explanations},
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = jun,
year = {2021},
note = {pp.~2294--2298},
doi = {https://doi.org/10.1109/CVPRW53098.2021.00259},
    entrysubtype = {conference},
}

@inproceedings{kayatani2021laughing,
author = {Yuta Kayatani and Zekun Yang and Mayu Otani and Noa Garcia and Chenhui Chu and Yuta Nakashima and Haruo Takemura},
title = {The laughing machine: Predicting humor in video},
booktitle = {Proc.~IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = jan,
year = {2021},
note = {pp.~2073--2082},
doi = {https://doi.org/10.1109/WACV48630.2021.00212},
    entrysubtype = {conference},
}

@inproceedings{ohashi2020idsou,
author = {Sora Ohashi and Tomoyuki Kajiwara and Chenhui Chu and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {{IDSOU} at {WNUT}-2020 {Task 2}: Identification of informative {COVID-19} English tweets},
booktitle = {Proc.~Workshop on Noisy User-Generated Text (W-NUT)},
month = 11,
year = {2020},
url = {http://dx.doi.org/10.18653/v1/2020.wnut-1.62},
note = {pp.~428--433},
    entrysubtype = {conference},
}

@inproceedings{otani2020uncovering,
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a},
title = {Uncovering hidden challenges in query-based video moment retrieval},
booktitle = {Proc.~British Machine Vision Conference (BMVC)},
month = sep,
year = {2020},
note = {12 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{garcia2020dataset,
author = {Noa Garcia and Chentao Ye and Zihua Liu and Qingtao Hu and Mayu Otani and Chenhui Chu and Yuta Nakashima and Teruko Mitamura},
title = {A dataset and baselines for visual question answering on art},
booktitle = {Proc.~European Conference on Computer Vision Workshops (VISARTS)},
month = aug,
year = {2020},
note = {pp.~92--108},
doi = {https://doi.org/10.1007/978-3-030-66096-3_8},
    entrysubtype = {conference},
}


@inproceedings{guo2020privacy,
author = {Zhiqiang Guo and Huigui Liu and Zhenzhong Kuang and Yuta Nakashima and Noboru Babaguchi},
title = {Privacy sensitive large-margin model for face de-identification},
booktitle = {Proc.~International Conference on Neural Computing for Advanced Applications (NCAA) },
month = aug,
year = {2020},
url = {https://doi.org/10.1007/978-981-15-7670-6_40},
note = {pp.~488--501},
    entrysubtype = {conference},
}


@inproceedings{li2020joint,
author = {Liangzhi Li and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
title = {Joint learning of vessel segmentation and artery/vein classification with post-processing},
booktitle = {Proc.~Medical Imaging with Deep Learning (MIDL)},
month = jun,
year = {2020},
note = {pp.~440--453},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{verma2020yoga,
author = {Manisha Verma and Sudhakar Kumawat and Yuta Nakashima and Shanmuganathan Raman},
title = {Yoga-82: A new dataset for fine-grained classification of human poses},
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = jun,
year = {2020},
url = {https://doi.org/10.1109/CVPRW50498.2020.00527},
note = {pp.~1038--1039},
doi = {https://doi.org/10.1109/CVPRW50498.2020.00527},
    entrysubtype = {conference},
}

@inproceedings{tanaka2020constructing,
author = {Koji Tanaka and Chenhui Chu and Haolin Ren and Benjamin Renoust and Yuta Nakashima and Noriko Takemura and Hajime Nagahara and Takao Fujikawa},
title = {Constructing a public meeting corpus},
booktitle = {Proc.~Conference on Language Resources and Evaluation (LREC)},
note = {pp.~1934-1940},
month = may,
year = {2020},
addendum = {(DOIなし)},
entrysubtype = {conference},
}

@inproceedings{nakashima2020toward,
author = {Yuta Nakashima and Hirokazu Kobori and Ryota Takaoka and Noriko Takemura and Tsukasa Kimura and Hajime Nagahara and Masayuki Numao and Kazumitsu Shinohara},
title = {Toward predicting learners' efficiency for adaptive e-learning},
booktitle = {Proc.~International Learning Analytics and Knowledge Conference (LAK)},
month = mar,
year = {2020},
note = {3 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{alizadeh2020video,
author = {Mehrasa Alizadeh and Shizuka Shirai and Noriko Takemura and Shogo Terai and Yuta Nakashima and Hajime Nagahara and Haruo Takemura},
title = {Video analytics in blended learning: Insights from learner-video interaction patterns},
booktitle = {Proc.~Workshop on Addressing Drop-Out Rates in Higher Education (ADORE)},
month = mar,
year = {2020},
note = {7 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{li2020iternet,
author = {Liangzhi Li and Manisha Verma and Yuta Nakashima and Hajime Nagahara and Ryo Kawasaki},
title = {{IterNet}: Retinal image segmentation utilizing structural redundancy in vessel networks},
booktitle = {Proc.~IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = mar,
year = {2020},
note = {pp.~3645--3654},
doi = {https://doi.org/10.1109/WACV45572.2020.9093621},
    entrysubtype = {conference},
}

@inproceedings{yang2020bert,
author = {Zekun Yang and Noa Garcia and Chenhui Chu and Mayu Otani and Yuta Nakashima and Haruo Takemura},
title = {{BERT} representations for video question answering},
booktitle = {Proc.~IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = mar,
year = {2020},
note = {pp.~1556--1565},
doi = {https://doi.org/10.1109/WACV45572.2020.9093596},
    entrysubtype = {conference},
}

@inproceedings{gacria2020knowit,
author = {Noa Garcia and Chenhui Chu and Mayu Otani and Yuta Nakashima},
title = {{KnowIT VQA}: Answering knowledge-based questions about videos},
booktitle = {Proc.~AAAI Conference Artificial Intelligence (AAAI)},
month = feb,
year = {2020},
note = {pp.~10826--10834},
addendum = {(\textbf{採択率: 20.6\%})},
doi = {https://doi.org/10.1609/aaai.v34i07.6713},
    entrysubtype = {conference},
}

@inproceedings{yamaguchi20203d,
author = {Takahiro Yamaguchi and Hajime Nagahara and Ken'ichi Morooka and Yuta Nakashima and Yuki Uranishi and Shoko Miyauchi and Ryo Kurazume},
title = {{3D} image reconstruction from multi-focus microscopic images},
booktitle = {Proc.~Pacific-Rim Symposium on Image and Video Technology (PSIVT)},
note = {pp.~73-85},
month = jan,
year = {2020},
url = {https://doi.org/10.1007/978-3-030-39770-8_6},
    entrysubtype = {conference},
}

@inproceedings{ashihara2019legal,
author = {Kazuki Ashihara and Chenhui Chu and Benjamin Renoust and Noriko Okubo and Noriko Takemura and Yuta Nakashima and Hajime Nagahara},
title = {Legal information as a complex network: Improving topic modeling through homophily},
booktitle = {Proc.~International Conference on Complex Networks and Their Applications},
note = {pp.~28-39},
month = nov,
year = {2019},
url = {https://doi.org/10.1007/978-3-030-36683-4_3},
    entrysubtype = {conference},
}

@inproceedings{sayo2019human,
author = {Akihiko Sayo and Hayato Onizuka and Diego Thomas and Yuta Nakashima and Hiroshi Kawasaki and Katsushi Ikeuchi},
title = {Human shape reconstruction with loose clothes from partially observed data by pose specific deformation},
booktitle = {Proc.~Pacific-Rim Symposium on Image and Video Technology (PSIVT)},
month = nov,
year = {2019},
note = {pp.~225--239},
addendum = {\textbf{[Best Paper Award]}},
doi = {https://doi.org/10.1007/978-3-030-34879-3_18},
    entrysubtype = {conference},
}

@inproceedings{otani2019adaptive,
author = {Mayu Otani and Chenhui Chu and Yuta Nakashima},
title = {Adaptive gating mechanism for identifying visually grounded paraphrases},
booktitle = {Proc.~Multi-Discipline Approach for Learning Concepts},
month = oct,
year = {2019},
note = {2 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
} 

@inproceedings{ben2019historical,
title = {Historical and modern features for Buddha statue classification},
author = {Benjamin Renoust and Matheus Oliveira Franca and Jacob Chan and Noa Garcia and Van Le and Ayaka Uesaka and Yuta Nakashima and Hajime Nagahara and Jueren Wang and Yutaka Fujioka},
year = {2019},
month = 10,
booktitle = {Proc.~Workshop on Structuring and Understanding of Multimedia HeritAge Contents},
note  = {pp.~23--30},
doi = {https://doi.org/10.1145/3347317.3357239},
    entrysubtype = {conference},
}

@inproceedings{verma2019facial,
author = {Manisha Verma and Hirokazu Kobori and Yuta Nakashima and Noriko Takemura and Hajime Nagahara},
title = {Facial expression recognition with skip-connection to leverage low-level features},
booktitle = {Proc.~IEEE International Conference Image Processing (ICIP)},
month = Sep,
year = {2019},
note = {pp.~51--55},
doi = {https://doi.org/10.1109/ICIP.2019.8803396},
    entrysubtype = {conference},
}

@inproceedings{renoust2019budaart,
author = {Benjamin Renoust and Matheus Oliveira Franca and Jacob Chan and Van Le and Ayaka Uesaka and Yuta Nakashima and Hajime Nagahara and Jueren Wang and Yutaka Fujioka},
title = {{BUDA.ART}: A multimodal content-based analysis and retrieval system for {Buddha} statues},
booktitle = {Proc.~ACM International Conference on Multimedia (MM)},
month = oct,
year = {2019},
note = {pp.~1062--1064},
addendum = {(Demo)},
doi = {https://doi.org/10.1145/3343031.3350591},
    entrysubtype = {conference},
}

@inproceedings{otani2019rethinking,
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a},
title = {Rethinking the evaluation of video summaries},
booktitle = {Proc.~IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = jun,
year = {2019},
note = {pp.~7596--7604},
addendum = {(\textbf{採択率: 25.0\%})},
doi = {https://doi.org/10.1109/CVPR.2019.00778},
    entrysubtype = {conference},
}

@inproceedings{garcia2019context,
author = {Noa Garcia and Benjamin Renoust and Yuta Nakashima},
title = {Context-aware embeddings for automatic art analysis},
booktitle = {Proc.~International Conference on Multimedia Retrieval (ICMR)},
month = jun,
year = {2019},
note = {pp.~25--33},
doi = {https://doi.org/10.1145/3323873.3325028},
    entrysubtype = {conference},
}

@inproceedings{shirai2019multimodal,
author = {Shizuka Shirai and Noriko Takemura and Yuta Nakashima and Hajime Nagahara and Haruo Takemura},
title = {Multimodal learning analytics: Society 5.0 project in Japan},
booktitle = {Proc.~International Conference on Learning Analytics and Knowledge (LAK)},
month = mar,
year = {2019},
note = {pp.~4--8},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{chu2018ipara,
author = {Chenhui Chu and Mayu Otani and Yuta Nakashima},
title = {{iParaphrasing}: Extracting visually grounded paraphrases via an image},
booktitle = {Proc.~International Conference on Computational Linguistics (COLING)},
month = aug,
year = {2018},
note = {pp.~3479--3492},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{kiura2018representing,
author = {Ryosuke Kimura and Akihiko Sayo and Fabian Lorenzo Dayrit and Yuta Nakashima and Hiroshi Kawasaki and Ambrosio Blanco and Katsushi Ikeuchi},
title = {Representing a partially observed non-rigid {3D} human using eigen-texture and eigen-deformation},
booktitle = {Proc.~International Conference on Pattern Recognition (ICPR)},
month = aug,
year = {2018},
note = {pp.~1043--1048},
doi = {https://doi.org/10.1109/ICPR.2018.8545658},
    entrysubtype = {conference},
}

@inproceedings{nakashima2017eigen,
author = {Yuta Nakashima and Fumio Okura and Norihiko Kawai and Hiroshi Kawasaki and Ambrosio Blanco and Katsushi Ikeuchi},
title = {Realtime novel view synthesis with eigen-texture regression},
booktitle = {Proc.~British Machine Vision Conference (BMVC)},
month = sep,
year = {2017},
note = {12 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}

@inproceedings{rongsirigul2017novel,
author = {Thiwat Rongsirigul and Yuta Nakashima and Tomokazu Sato and Naokazu Yokoya},
title = {Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic {HMD}},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jul,
year = {2017},
note = {pp.~703--708},
doi = {https://doi.org/10.1109/ICME.2017.8019417},
    entrysubtype = {conference},
}

@inproceedings{dayrit2017remagicmirroir,
author = {Fabian Lorenzo Dayrit and Ryosuke Kimura and Yuta Nakashima and Ambrosio Blanco and Hiroshi Kawasaki and Katsushi Ikeuchi},
title = {{ReMagicMirror}: Action learning using human reenactment with the mirror metaphor},
booktitle = {Proc.~International Conference on Multimedia Modeling (MMM)},
month = jan,
year = {2017},
note = {pp.~303--315},
doi = {https://doi.org/10.1007/978-3-319-51811-4_25},
    entrysubtype = {conference},
} 

@inproceedings{otani2016video,
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a and Naokazu Yokoya},
title = {Video summarization using deep semantic features},
booktitle = {Proc.~Asian Conference on Computer Vision (ACCV)},
month = sep,
year = {2016},
note = {16 pages},
doi = {https://doi.org/10.1007/978-3-319-54193-8_23},
    entrysubtype = {conference},
}

@inproceedings{otani2016learning,
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a and Naokazu Yokoya},
title = {Learning joint representations of videos and sentences with web image search},
booktitle = {Proc.~Workshop on Web-scale Vision and Social Media},
month = aug,
year = {2016},
note = {pp.~651-667},
doi = {https://doi.org/10.1007/978-3-319-46604-0_46},
    entrysubtype = {conference},
}

@inproceedings{tejerodepablo2016human,
author = {Antonio Tejero-de-Pablos and Yuta Nakashima and Tomokazu Sato and Naokazu Yokoya},
title = {Human action recognition-based video summarization for {RGB-D} personal sports video},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jul,
year = {2016},
note = {6 pages},
doi = {https://doi.org/10.1109/ICME.2016.7552938},
    entrysubtype = {conference},
}

@inproceedings{takehara20163d,
  title={{3D} shape template generation from {RGB-D} images capturing a moving and deforming object},
  author={Takehara, Hikari and Yuta Nakashima and Sato, Tomokazu and Yokoya, Naokazu},
  booktitle={Proc.~Electronic Imaging},
  volume={2016},
  number={21},
  note = {pp.~3DIPM-407.1--3DIPM-407.7},
  year={2016},
  month=feb,
    doi = {https://doi.org/10.2352/ISSN.2470-1173.2016.21.3DIPM-407},
    entrysubtype = {conference},
}

@inproceedings{otani2015textual,
author = {Mayu Otani and Yuta Nakashima and Tomokazu Sato and Naokazu Yokoya},
title = {Textual description-based video summarization for video blogs},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jun,
year = {2015},
note = {6 pages},
doi = {https://doi.org/10.1109/ICME.2015.7177493},
    entrysubtype = {conference},
}

@inproceedings{nakashima2015facial,
author = {Yuta Nakashima and Tatsuya Koyama and Naokazu Yokoya and Noboru Babaguchi},
title = {Facial expression preserving privacy protection using image melding},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jun,
year = {2015},
note = {6 pages},
doi = {https://doi.org/10.1109/ICME.2015.7177394},
    entrysubtype = {conference},
}

@inproceedings{dayrit2014free,
author = {Fabian Lorenzo Dayrit and Yuta Nakashima and Tomokazu Sato and Naokazu Yokoya},
title = {Free-viewpoint {AR} human-motion reenactment based on a single {RGB-D} video stream},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jul,
year = {2014},
note = {6 pages},
doi = {https://doi.org/10.1109/ICME.2014.6890243},
    entrysubtype = {conference},
}

@inproceedings{nakashima2013augmented,
author = {Yuta Nakashima and Yusuke Uno and Norihiko Kawai and Tomokazu Sato and Naokazu Yokoya},
title = {Augmented reality image generation with virtualized real objects using view-dependent texture and geometry},
booktitle = {Proc.~IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
month = oct,
year = {2013},
note = {6 pages},
doi = {https://doi.org/10.1109/ISMAR.2013.6671827},
    entrysubtype = {conference},
}

@inproceedings{nakashima2013inferring,
author = {Yuta Nakashima and Naokazu Yokoya},
title = {Inferring what the videographer wanted to capture},
booktitle = {Proc.~IEEE International Conference on Image Processing (ICIP)},
month = sep,
year = {2013},
note = {pp.~191--195},
addendum = {\textbf{[Open Paper Award]}},
doi = {https://doi.org/10.1109/ICIP.2013.6738040},
    entrysubtype = {conference},
}

@inproceedings{koyama2013realtime,
author = {Tatsuya Koyama and Yuta Nakashima and Noboru Babaguchi},
title = {Real-time privacy protection system for social videos using intentionally-captured persons detection},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jul,
year = {2013},
note = {6 pages},
    entrysubtype = {conference},
}

@inproceedings{koyama2012markov,
author = {Tatsuya Koyama and Yuta Nakashima and Noboru Babaguchi},
title = {Markov random field-based real-time detection of intentionally-captured persons},
booktitle = {Proc.~IEEE International Conference on Image Processing (ICIP)},
month = sep,
year = {2012},
note = {pp.~1377--1380},
doi = {https://doi.org/10.1109/ICME.2013.6607622},
    entrysubtype = {conference},
}

@inproceedings{nakashima2011extracting,
author = {Yuta Nakashima and Noboru Babaguchi},
title = {Extracting intentionally captured regions using point trajectories},
booktitle = {Proc.~ACM International Conference on Multimedia (MM)},
month = nov,
year = {2011},
note = {pp.~1417--1420},
doi = {https://doi.org/10.1145/2072298.2072029},
    entrysubtype = {conference},
}

@inproceedings{nakashima2011automatic,
author = {Yuta Nakashima and Noboru Babaguchi and Jianping Fan},
title = {Automatic generation of privacy-protected videos using background estimation},
booktitle = {Proc.~IEEE International Conference on Multimedia and Expo (ICME)},
month = jul,
year = {2011},
note = {6 pages},
doi = {https://doi.org/10.1109/ICME.2011.6011955},
    entrysubtype = {conference},
}

@inproceedings{nakashima2010automatically,
author = {Yuta Nakashima and Noboru Babaguchi and Jianping Fan},
title = {Automatically protecting privacy in consumer generated videos using intended human object detector},
booktitle = {Proc.~ACM International Conference on Multimedia (MM)},
month = oct,
year = {2010},
note = {pp.~1135--1138},
doi = {https://doi.org/10.1145/1873951.1874169},
    entrysubtype = {conference},
}

@inproceedings{kaneto2010realtime,
author = {Ryosuke Kaneto and Yuta Nakashima and Noboru Babaguchi},
title = {Real-time user position estimation in indoor environments using digital watermarking for audio signals},
booktitle = {Proc.~International Conference on Pattern Recognition (ICPR)},
month = aug,
year = {2010},
note = {pp.~97--100},
doi = {https://doi.org/10.1109/ICPR.2010.32},
    entrysubtype = {conference},
}

@inproceedings{uegaki2010discriminating,
author = {Hiroshi Uegaki and Yuta Nakashima and Noboru Babaguchi},
title = {Discriminating intended human objects in consumer videos},
booktitle = {Proc.~International Conference on Pattern Recognition (ICPR)},
month = aug,
year = {2010},
note = {pp.~4380--4383},
doi = {https://doi.org/10.1109/ICPR.2010.1065},
    entrysubtype = {conference},
}


@inproceedings{huckle2020,
author = {Nikolai Huckle and Noa Garcia and Yuta Nakashima},
title = {Demographic Influences on Contemporary Art with Unsupervised Style Embeddings},
booktitle = {Proc.~European Conference on Computer Vision Workshops (VISARTS)},
month = aug,
year = {2020},
note = {pp.~126--142},
doi = {https://doi.org/10.1007/978-3-030-66096-3_10},
    entrysubtype = {conference},
}


@inproceedings{garcia2020knowledgea,
author = {Noa Garcia and Yuta Nakashima},
title = {Knowledge-based video question answering with unsupervised scene descriptions},
booktitle = {Proc.~European Conference on Computer Vision (ECCV)},
month = aug,
year = {2020},
note = {pp.~581--598},
addendum = {(\textbf{採択率: 27.1\%})},
doi = {https://doi.org/10.1007/978-3-030-58523-5_34},
    entrysubtype = {conference},
}


@inproceedings{otani2017demo,
title = {Video question answering to find a desired video segment},
author = {Mayu Otani and Yuta Nakashima and Esa Rahtu and Janne Heikkil\"a}, 
year = {2017},
month = {8},
booktitle = {Proc.~Open Knowledge Base and Question Answering Workshop (OKBQA)},
note = {2 pages},
addendum = {(DOIなし)},
    entrysubtype = {conference},
}


@inproceedings{garcia2020women,
    author = {Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima},
    title = {Knowledge-Based Visual Question Answering in Videos},
    booktitle = {Proc.~Workshop on Women in Computer Vision},
    month = 6,
    year = {2020},
    note = {3 pages},
    addendum = {(DOIなし)},
    entrysubtype = {conference},
}